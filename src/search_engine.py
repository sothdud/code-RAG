"""
Memgraph ê¸°ë°˜ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
Vector(ì˜ë¯¸ ê²€ìƒ‰) + BM25(í‚¤ì›Œë“œ ê²€ìƒ‰) + Graph(ë§¥ë½/íë¦„ ê²€ìƒ‰)
"""
import re
from rank_bm25 import BM25Okapi
from qdrant_client.models import Filter, FieldCondition, MatchText
from .database import VectorStore
from .graph_store import GraphStore
from loguru import logger

class SmartSearchEngine:

    def __init__(self, vector_store: VectorStore, graph_store: GraphStore):
        self.db = vector_store
        self.graph = graph_store
        
        # ---------------------------------------------------------
        # ğŸš€ [NEW] BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ë¡œë“œ)
        # ---------------------------------------------------------
        logger.info("â³ Initializing BM25 Index from Vector Store...")
        
        self.all_chunks = self._fetch_all_docs_from_db()
        
        if self.all_chunks:
            tokenized_corpus = [self._tokenize_code(doc.get('content', '')) for doc in self.all_chunks]
            self.bm25 = BM25Okapi(tokenized_corpus)
            logger.success(f"âœ… BM25 Index Ready! (Loaded {len(self.all_chunks)} chunks)")
        else:
            logger.warning("âš ï¸ No data found in Qdrant. BM25 will be disabled until data is ingested.")
            self.bm25 = None

    def _tokenize_code(self, text: str):
        """
        ì½”ë“œìš© í† í¬ë‚˜ì´ì €: snake_case, CamelCase, íŠ¹ìˆ˜ë¬¸ì ë“±ì„ ë¶„ë¦¬í•˜ì—¬ ì¸ë±ì‹±
        ì˜ˆ: "INVALID_KEY" -> ["invalid", "key"]
        """
        clean_text = re.sub(r"[_\.\(\)\[\]\{\}\=\:\,\;\"\'\/]", " ", text)
        return clean_text.lower().split()

    def _fetch_all_docs_from_db(self):
        """Qdrantì—ì„œ ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            all_points = []
            offset = None
            while True:
                points, offset = self.db.client.scroll(
                    collection_name=self.db.collection,
                    limit=100,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                for p in points:
                    if p.payload:
                        all_points.append(p.payload)
                
                if offset is None:
                    break
            return all_points
        except Exception as e:
            logger.error(f"âš ï¸ Failed to fetch docs for BM25: {e}")
            return []

    def reciprocal_rank_fusion(self, vector_results, bm25_results, k=60):
        """
        ğŸ§¬ RRF ì•Œê³ ë¦¬ì¦˜: ë‘ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ í•©ì‚°í•˜ì—¬ ì¬ì •ë ¬
        Score = 1 / (k + rank)
        """
        fusion_scores = {}

        # 1. Vector ê²°ê³¼ ì ìˆ˜ ë§¤ê¸°ê¸°
        for rank, item in enumerate(vector_results):
            payload = item.payload if hasattr(item, 'payload') else item
            doc_id = payload.get('qualified_name') or payload.get('filepath')
            
            if not doc_id: continue

            if doc_id not in fusion_scores:
                fusion_scores[doc_id] = {'doc': payload, 'score': 0}
            fusion_scores[doc_id]['score'] += 1 / (k + rank)

        # 2. BM25 ê²°ê³¼ ì ìˆ˜ ë§¤ê¸°ê¸°
        for rank, item in enumerate(bm25_results):
            doc_id = item.get('qualified_name') or item.get('filepath')
            
            if not doc_id: continue

            if doc_id not in fusion_scores:
                fusion_scores[doc_id] = {'doc': item, 'score': 0}
            fusion_scores[doc_id]['score'] += 1 / (k + rank)

        # 3. ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬
        sorted_results = sorted(fusion_scores.values(), key=lambda x: x['score'], reverse=True)
        
        return [item['doc'] for item in sorted_results]

    def _extract_filenames(self, query: str) -> list[str]:
        """ì§ˆë¬¸ì—ì„œ .py íŒŒì¼ëª…ë“¤ì„ ì¶”ì¶œ"""
        return re.findall(r'\b[\w-]+\.py\b', query)
    
    def _extract_function_names(self, query: str) -> list[str]:
        """
        ì§ˆë¬¸ì—ì„œ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª…ì„ ì¶”ì¶œ
        ì˜ˆ: "predict_tree_klarfê°€ ë­í•´?" -> ["predict_tree_klarf"]
        """
        # Python í•¨ìˆ˜ëª… íŒ¨í„´: snake_case, camelCase
        pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]{1,49})\b'
        candidates = re.findall(pattern, query)
        
        # ì¼ë°˜ ì˜ë‹¨ì–´ ì œì™¸
        common_words = {'what', 'does', 'how', 'why', 'where', 'when', 
                       'function', 'class', 'method', 'file', 'code', 'this',
                       'that', 'the', 'is', 'are', 'do', 'can', 'will', 'from'}
        
        function_names = []
        for c in candidates:
            # ìµœì†Œ 3ì ì´ìƒì´ê±°ë‚˜ ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨
            if (len(c) >= 3 or '_' in c) and c.lower() not in common_words:
                function_names.append(c)
        
        return function_names
    
    def _get_exact_function_chunks(self, function_names: list[str]) -> list:
        """
        â­ í•¨ìˆ˜ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì²­í¬ë“¤ì„ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
        (ê²€ìƒ‰ ìˆœìœ„ì™€ ë¬´ê´€í•˜ê²Œ ë°˜ë“œì‹œ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•¨)
        """
        if not function_names or not self.all_chunks:
            return []
        
        exact_matches = []
        seen_qns = set()  # ì¤‘ë³µ ë°©ì§€
        
        for chunk in self.all_chunks:
            chunk_name = chunk.get('name', '')
            qn = chunk.get('qualified_name', '')
            
            if qn in seen_qns:
                continue
            
            # í•¨ìˆ˜ëª…ì´ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ qualified_name ëë¶€ë¶„ì´ ì¼ì¹˜
            for target_name in function_names:
                if chunk_name == target_name or qn.endswith(f'.{target_name}'):
                    exact_matches.append(chunk)
                    seen_qns.add(qn)
                    logger.info(f"  âœ… Exact match found: {qn}")
                    break
        
        return exact_matches

    def search(self, query: str, top_k: int = 5):
        """
        [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸]
        1. â­ Exact Function Name Matching (NEW!)
        2. Keyword Search (BM25): ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­
        3. Vector Search (Dense): ì˜ë¯¸ì  ìœ ì‚¬ì„±
        4. RRF Fusion: ìˆœìœ„ í˜¼í•©
        5. Reranking (Cross-Encoder): ì •ë°€ ì¬ê²€ì¦
        6. Context Expansion: Graph DB ë¬¸ë§¥ ë³´ê°•
        """
        print(f"ğŸ” Hybrid Searching for: '{query}'")

        # ---------------------------------------------------------
        # â­ 0. í•¨ìˆ˜ëª… ì •í™• ë§¤ì¹­ (NEW!)
        # ---------------------------------------------------------
        target_functions = self._extract_function_names(query)
        exact_function_chunks = []
        
        if target_functions:
            print(f"  ğŸ¯ Detected function names: {target_functions}")
            exact_function_chunks = self._get_exact_function_chunks(target_functions)
            print(f"  âœ… Found {len(exact_function_chunks)} exact matches")
        
        # 1. íŒŒì¼ëª… í•„í„° í™•ì¸
        target_files = self._extract_filenames(query)
        search_filters = None
        if target_files:
            print(f"  ğŸ“‚ Filter by files: {target_files}")
            search_filters = Filter(
                should=[
                    FieldCondition(
                        key="filepath", 
                        match=MatchText(text=fname)
                    ) for fname in target_files
                ]
            )

        # ---------------------------------------------------------
        # 2. Vector Search (Dense)
        # ---------------------------------------------------------
        vector_candidates = self.db.search(query, top_k=top_k * 4, query_filter=search_filters)
        
        # ---------------------------------------------------------
        # 3. Keyword Search (BM25)
        # ---------------------------------------------------------
        bm25_candidates = []
        if self.bm25:
            tokenized_query = self._tokenize_code(query)
            bm25_candidates = self.bm25.get_top_n(tokenized_query, self.all_chunks, n=top_k * 4)

        # ---------------------------------------------------------
        # 4. RRF (Reciprocal Rank Fusion) ê²°í•©
        # ---------------------------------------------------------
        print(f"  ğŸ§¬ Fusing: Vector({len(vector_candidates)}) + BM25({len(bm25_candidates)})")
        
        candidates_before_rerank = self.reciprocal_rank_fusion(
            vector_candidates, 
            bm25_candidates, 
            k=60
        )

        # ---------------------------------------------------------
        # â­ 4.5 ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìµœìƒìœ„ì— ì‚½ì… (NEW!)
        # ---------------------------------------------------------
        if exact_function_chunks:
            # ì¤‘ë³µ ì œê±°: exact matchê°€ ì´ë¯¸ í›„ë³´ì— ìˆìœ¼ë©´ ì œê±°
            exact_qns = {c.get('qualified_name') for c in exact_function_chunks}
            candidates_before_rerank = [
                c for c in candidates_before_rerank 
                if c.get('qualified_name') not in exact_qns
            ]
            
            # ì •í™• ë§¤ì¹­ì„ ë§¨ ì•ì— ì¶”ê°€
            candidates_before_rerank = exact_function_chunks + candidates_before_rerank
            print(f"  ğŸ¯ Exact matches promoted to top!")

        # ---------------------------------------------------------
        # 5. Reranking (Cross-Encoder)
        # ---------------------------------------------------------
        slice_for_rerank = candidates_before_rerank[:20]
        
        if slice_for_rerank:
            print(f"  âš–ï¸ Reranking top {len(slice_for_rerank)} candidates...")
            
            # â­ ê°œì„ : ì •í™• ë§¤ì¹­ í•¨ìˆ˜ëŠ” rerankì—ì„œë„ ë†’ì€ ìš°ì„ ìˆœìœ„ ìœ ì§€
            if exact_function_chunks:
                # ì •í™• ë§¤ì¹­ì€ ë¬´ì¡°ê±´ í¬í•¨
                non_exact = [c for c in slice_for_rerank if c not in exact_function_chunks]
                # rerankëŠ” ë‚˜ë¨¸ì§€ì—ë§Œ ì ìš©
                reranked_rest = self.db.rerank(query, non_exact, top_k=max(1, top_k - len(exact_function_chunks)))
                final_results = exact_function_chunks + reranked_rest
                # top_k ê°œìˆ˜ ë§ì¶”ê¸°
                final_results = final_results[:top_k]
            else:
                final_results = self.db.rerank(query, slice_for_rerank, top_k=top_k)
        else:
            final_results = []

        if not final_results:
            print("  âš ï¸ No candidates found.")
            return []

        # ---------------------------------------------------------
        # 6. Graph Context Expansion
        # ---------------------------------------------------------
        enhanced_results = []

        for i, payload in enumerate(final_results):
            qualified_name = payload.get('qualified_name')

            context_entry = {
                "chunk": payload,
                "flow_context": [],
                "related_code": [],
            }

            if qualified_name:
                # 1. ì‹¤í–‰ íë¦„ ê°€ì ¸ì˜¤ê¸° (Graph)
                context_entry["flow_context"] = self.graph.get_execution_flow(qualified_name, depth=2)

                # 2. ìƒìœ„ 3ê°œë§Œ Callee ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
                if i < 3:
                    callees = self.graph.get_callees(qualified_name)
                    if callees:
                        context_entry['related_code'] = self.db.retrieve_by_filenames(callees)

            enhanced_results.append(context_entry)

        return enhanced_results