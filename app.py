# app.py
import streamlit as st
import os
import json
import re
from dotenv import load_dotenv

# ê¸°ì¡´ ëª¨ë“ˆ
from src.database import VectorStore
from src.llm import LocalLLM
from src.graph_store import GraphStore
from src.search_engine import SmartSearchEngine
from src import prompts 

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# =====================================================================
# [ë””ìì¸ CSS] 
# =====================================================================
st.markdown("""
    <style>
        /* ë“œë¡­ë‹¤ìš´ ê¾¸ë¯¸ê¸° */
        div[data-baseweb="select"] > div {
            background-color: #f8f9fa;
            border: 2px solid #4a90e2;
            border-radius: 8px;
        }
        /* ì±„íŒ… ë§í’ì„  ì—¬ë°± ì¡°ì • */
        .stChatMessage { padding: 1rem; }
    </style>
""", unsafe_allow_html=True)

# =====================================================================
# [ì§ˆë¬¸ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ìƒì„± ë¡œì§]
# =====================================================================

def detect_query_type(query: str, llm) -> dict:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë™ì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í™•ì¥"""
    
    # 7b ëª¨ë¸ì„ ìœ„í•œ ë‚ ë µí•œ í”„ë¡¬í”„íŠ¸ (ì˜¤ë²„í”¼íŒ… ë°©ì§€)
    router_prompt = """
    ë‹¹ì‹ ì€ C# WPF(MVVM íŒ¨í„´)ì™€ Pythonìœ¼ë¡œ êµ¬ì„±ëœ í”„ë¡œì íŠ¸ì˜ RAG ê²€ìƒ‰ ë¼ìš°í„°ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§§ê³  ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ì‹¤ì œ ì†ŒìŠ¤ ì½”ë“œ(C#, XAML, Python)ì— ì¡´ì¬í•  ë²•í•œ 
    'ê°€ìƒì˜ í•µì‹¬ ê¸°ìˆ  í‚¤ì›Œë“œ'ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.

    [ì§€ì¹¨]
    1. ì§ˆë¬¸ì˜ ì •ë‹µì´ ë  ì½”ë“œì— ì–´ë–¤ í´ë˜ìŠ¤, ë©”ì„œë“œ, ì†ì„±, XAML íƒœê·¸ê°€ ìˆì„ì§€ ìƒìƒí•˜ì„¸ìš”.
    2. ì§ˆë¬¸ì´ í•œêµ­ì–´ì—¬ë„, ê²€ìƒ‰ìš© í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ê´€ë ¨ëœ ì˜ë¬¸ í”„ë¡œê·¸ë˜ë° ìš©ì–´(ì˜ˆ: Button, Command, ViewModel, Event ë“±)ë¡œ í™•ì¥í•˜ì„¸ìš”.

    [ì¶œë ¥ í˜•ì‹ (JSON)]
    {
        "type": "bug|flow|search|mvvm|general",
        "filenames": ["ì˜ˆìƒë˜ëŠ”_íŒŒì¼ëª….py", "íŒŒì¼ëª….cs"],
        "target_name": "ì˜ˆìƒë˜ëŠ”_í•¨ìˆ˜_ë˜ëŠ”_í´ë˜ìŠ¤ëª…(ì—†ìœ¼ë©´ null)",
        "language": "python|csharp|xaml|mixed",
        "search_keywords": "ê²€ìƒ‰ì—”ì§„ì— ë„£ì„ êµ¬ì²´ì ì´ê³  í™•ì¥ëœ ì˜ë¬¸ í‚¤ì›Œë“œ ëª¨ìŒ (ì˜ˆ: Train Stop Button Command Execute ViewModel XAML Binding)"
    }
    
    ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    """

    try:
        # ğŸŒŸ í•µì‹¬: ë¼ìš°í„°ëŠ” 7b ëª¨ë¸(fast)ì„ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œ!
        response_gen = llm.generate_response(router_prompt, query, use_fast=True)
        response_text = "".join(list(response_gen))
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ë°±í‹± ë“± ì œê±°)
        clean_json = response_text.replace("```json", "").replace("```", "").strip()
        result = json.loads(clean_json)
        
        # í™•ì¥ëœ í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ í´ë°±
        if 'search_keywords' not in result or not result['search_keywords']:
            result['search_keywords'] = query
            
        return result

    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "type": "general",
            "filenames": [],
            "target_name": None,
            "language": "mixed",
            "search_keywords": query # ì‹¤íŒ¨í•´ë„ ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ì§„í–‰
        }

def build_optimized_prompt(query: str, results: list, query_info: dict) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° ì•ˆì „í•˜ê²Œ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    context_texts = []
    
    # ğŸŒŸ ì–´ë–¤ í˜•íƒœì˜ ë°ì´í„°(ê°ì²´/ë”•ì…”ë„ˆë¦¬)ê°€ ë“¤ì–´ì˜¤ë“  ì•ˆì „í•˜ê²Œ íŒŒì‹±í•˜ë„ë¡ ì „ë©´ ìˆ˜ì •
    for r in results:
        filepath = "Unknown"
        content = ""
        
        if hasattr(r, 'payload') and isinstance(r.payload, dict):
            filepath = r.payload.get('filepath') or r.payload.get('chunk', {}).get('filepath') or "Unknown"
            content = r.payload.get('content') or r.payload.get('chunk', {}).get('content') or ""
        elif isinstance(r, dict):
            filepath = r.get('filepath') or r.get('chunk', {}).get('filepath') or "Unknown"
            content = r.get('content') or r.get('chunk', {}).get('content') or ""
        else:
            content = str(r)
            
        context_texts.append(f"File: {filepath}\nCode:\n{content}")
        
    context_str = "\n\n---\n\n".join(context_texts)
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
    qtype = query_info.get('type', 'general')
    
    if qtype == 'existence' and hasattr(prompts, 'get_existence_check_prompt'):
        return prompts.get_existence_check_prompt(query, context_str, query_info.get('target_name') or "unknown")
    elif qtype == 'flow' and hasattr(prompts, 'get_flow_analysis_prompt'):
        return prompts.get_flow_analysis_prompt(query, context_str)
    elif qtype == 'bug' and hasattr(prompts, 'get_bug_analysis_prompt'):
        return prompts.get_bug_analysis_prompt(query, context_str)
    elif qtype == 'mvvm' and hasattr(prompts, 'get_mvvm_analysis_prompt'):
        return prompts.get_mvvm_analysis_prompt(query, context_str)
    elif qtype == 'file_summary' and hasattr(prompts, 'get_file_summary_prompt'):
        return prompts.get_file_summary_prompt(query, context_str, query_info.get('filename') or "Multiple Files")
    elif qtype == 'error' and hasattr(prompts, 'get_error_diagnostic_prompt'):
        traceback_match = re.search(r'(Traceback.*?)(?:\n\n|\Z)', query, re.DOTALL)
        traceback = traceback_match.group(1) if traceback_match else query
        return prompts.get_error_diagnostic_prompt(query, context_str, traceback, language=query_info.get('language', 'python'))
    
    # í•´ë‹¹ í”„ë¡¬í”„íŠ¸ê°€ ì—†ê±°ë‚˜ generalì¼ ê²½ìš° ê¸°ë³¸ ìš”ì•½/ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if hasattr(prompts, 'get_general_prompt'):
        return prompts.get_general_prompt(query, context_str)
    
    return prompts.get_file_summary_prompt(query, context_str, "Multiple Files")


# =====================================================================
# ì•± ê¸°ë³¸ ì„¤ì • ë° ì„¸ì…˜ ì´ˆê¸°í™”
# =====================================================================
PROJECT_MAP = {
    "ğŸŒ TIDAL codebase": "semiconductor_codebase_tidal",
    "Dreamer codebase": "semiconductor_codebase",
}

st.title("GraphRAG")

with st.sidebar:
    st.header("ì„¤ì •")
    selected_project = st.selectbox(
        "ì¡°íšŒí•  í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:", 
        options=list(PROJECT_MAP.keys())
    )
    target_collection = PROJECT_MAP[selected_project]
    
    st.caption(f"í˜„ì¬ ì—°ê²°ëœ Qdrant ì»¬ë ‰ì…˜:\n`{target_collection}`")
    
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# ì—”ì§„ ë¡œë“œ
if "current_collection" not in st.session_state or st.session_state.current_collection != target_collection:
    st.session_state.current_collection = target_collection
    st.session_state.messages = [] 
    
    with st.spinner(f"'{selected_project}' ì—”ì§„ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... â³"):
        st.session_state.vector_store = VectorStore(collection_name=target_collection)
        st.session_state.graph_store = GraphStore()
        st.session_state.search_engine = SmartSearchEngine(
            st.session_state.vector_store, 
            st.session_state.graph_store
        )
        st.session_state.llm = LocalLLM()
        
# =====================================================================
# ë©”ì¸ ì±„íŒ… UI
# =====================================================================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

query = st.chat_input("ì½”ë“œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! (ì˜ˆ: ì´ XAML ë²„íŠ¼ í´ë¦­í•˜ë©´ C#ì˜ ì–´ë–¤ ë©”ì„œë“œê°€ ì‹¤í–‰ë¼?)")

if query:
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        engine = st.session_state.search_engine
        llm = st.session_state.llm
        
        # 1. ì˜ë„ íŒŒì•…
        with st.spinner("ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”"):
            query_info = detect_query_type(query, llm)
        
        # 2. ì½”ë“œ ê²€ìƒ‰
        with st.spinner(f"[{query_info.get('type', 'general').upper()}] ê´€ë ¨ ì½”ë“œë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤... ğŸ§"):
            results = engine.search(query, top_k=5)
            
            if not results:
                answer = "âŒ ê´€ë ¨ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                st.error(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
            else:
                # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë‹µë³€ ì¶œë ¥
                prompt = build_optimized_prompt(query, results, query_info)
                
                # write_streamì€ ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                answer = st.write_stream(llm.generate_response(prompt, query))
                
                # 4. ì°¸ì¡° íŒŒì¼ ëª©ë¡ UI (ì•ˆì „í•˜ê²Œ íŒŒì‹±)
                with st.expander("ğŸ“š ì°¸ì¡°ëœ íŒŒì¼ ëª©ë¡ ë³´ê¸°"):
                    st.write(f"ğŸ’¡ ê°ì§€ëœ ì§ˆë¬¸ ìœ í˜•: `{query_info.get('type', 'Unknown').upper()}`")
                    
                    ref_files = set()
                    for r in results:
                        if hasattr(r, 'payload') and isinstance(r.payload, dict):
                            file_path = r.payload.get('filepath') or r.payload.get('chunk', {}).get('filepath')
                            if file_path: ref_files.add(file_path)
                        elif isinstance(r, dict):
                            file_path = r.get('filepath') or r.get('chunk', {}).get('filepath')
                            if file_path: ref_files.add(file_path)
                            
                    for f in sorted(filter(None, ref_files)):
                        st.write(f"- `{f}`")

                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": answer})