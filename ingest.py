import os
import json
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import track

# Qdrant Filter ê´€ë ¨ ì„í¬íŠ¸ (ì‚­ì œìš©)
from qdrant_client.models import Filter, FieldCondition, MatchText

from src.parser import ASTParser
from src.graph_builder import GraphBuilder
from src.database import VectorStore
from src.graph_store import GraphStore
from src.path_utils import should_skip_path

load_dotenv()
console = Console()

STATE_FILE = ".ingest_state.json"

def calculate_file_hash(filepath: Path) -> str:
    """íŒŒì¼ ë‚´ìš©ì˜ MD5 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except FileNotFoundError:
        return ""

def load_state() -> dict:
    """ì´ì „ ì¸ë±ì‹± ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_state(state: dict):
    """í˜„ì¬ ì¸ë±ì‹± ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)

def main():
    # 1. ì„¤ì •
    TARGET_DIR = Path(os.getenv("SOURCE_CODE_PATH", "./data"))
    if not TARGET_DIR.exists():
        console.print(f"[red]âŒ ê²½ë¡œ ì—†ìŒ: {TARGET_DIR}[/red]")
        return

    parser = ASTParser()
    graph_builder = GraphBuilder()
    db = VectorStore()
    graph_store = GraphStore()
    
    # 2. ìƒíƒœ ë¡œë“œ ë° ë³€ê²½ ê°ì§€
    console.print(f"\n[bold yellow]ğŸ” Phase 1: Detecting Changes...[/bold yellow]")
    
    previous_state = load_state()
    current_state = {}
    
    all_files = []
    # íŒŒì¼ íƒìƒ‰
    for root, dirs, files in os.walk(TARGET_DIR):
        root_path = Path(root)
        
        # ë””ë ‰í† ë¦¬ ì œì™¸ ì²˜ë¦¬ (ì¬ê·€ì  íƒìƒ‰ íš¨ìœ¨í™”)
        dirs[:] = [d for d in dirs if not should_skip_path(root_path / d, TARGET_DIR)]
        
        for file in files:
            if not file.endswith('.py'): continue
            file_path = root_path / file
            if should_skip_path(file_path, TARGET_DIR): continue
            
            all_files.append(file_path)

    # ë³€ê²½ ì‚¬í•­ ë¶„ë¥˜
    files_to_embed = []      # ì„ë² ë”© ìƒˆë¡œ í•´ì•¼ í•  íŒŒì¼ (ì‹ ê·œ/ìˆ˜ì •)
    files_to_delete = []     # DBì—ì„œ ì§€ì›Œì•¼ í•  íŒŒì¼ (ìˆ˜ì •/ì‚­ì œ)
    unchanged_files = []     # ë³€ê²½ ì—†ëŠ” íŒŒì¼
    
    # 2-1. ì‹ ê·œ ë° ìˆ˜ì • íŒŒì¼ ê°ì§€
    for file_path in all_files:
        str_path = str(file_path)
        current_hash = calculate_file_hash(file_path)
        current_state[str_path] = current_hash
        
        prev_hash = previous_state.get(str_path)
        
        if prev_hash != current_hash:
            if prev_hash is None:
                console.print(f"  [green]+ New:[/green] {file_path.name}")
            else:
                console.print(f"  [yellow]* Modified:[/yellow] {file_path.name}")
                files_to_delete.append(str_path) # ìˆ˜ì •ëœ ê²½ìš° ê¸°ì¡´ ê±° ì‚­ì œ í•„ìš”
            files_to_embed.append(file_path)
        else:
            unchanged_files.append(file_path)

    # 2-2. ì‚­ì œëœ íŒŒì¼ ê°ì§€
    for old_path in previous_state:
        if old_path not in current_state:
            console.print(f"  [red]- Deleted:[/red] {old_path}")
            files_to_delete.append(old_path)

    # ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (Graph ì¬ìƒì„±ì€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ë³´í†µ ì½”ë“œ ë³€ê²½ ì—†ìœ¼ë©´ Graphë„ ê·¸ëŒ€ë¡œ)
    if not files_to_embed and not files_to_delete:
        console.print("\n[bold green]âœ… No changes detected. System is up to date.[/bold green]")
        return

    # ---------------------------------------------------------
    # 3. ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ (Incremental)
    # ---------------------------------------------------------
    # ì£¼ì˜: db.recreate_collection()ì€ ì´ì œ ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ í•˜ê±°ë‚˜, 
    # ë³„ë„ ì˜µì…˜ìœ¼ë¡œ ë¹¼ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìë™ ì‚­ì œ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    
    if files_to_delete:
        console.print(f"\n[bold red]ğŸ—‘ï¸ Removing obsolete chunks ({len(files_to_delete)} files)...[/bold red]")
        # Qdrantì—ì„œ íŒŒì¼ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ
        for file_path in files_to_delete:
            try:
                db.client.delete(
                    collection_name=db.collection,
                    points_selector=Filter(
                        must=[
                            FieldCondition(
                                key="filepath",
                                match=MatchText(text=file_path)
                            )
                        ]
                    )
                )
            except Exception as e:
                console.print(f"  âš ï¸ Failed to delete {file_path}: {e}")

    # ---------------------------------------------------------
    # 4. íŒŒì‹± ë° ê·¸ë˜í”„ ë¹Œë“œ (Hybrid)
    # ---------------------------------------------------------
    # Graph ì—°ê²°ì„±(Caller/Callee)ì„ ìœ„í•´ 'êµ¬ì¡°'ëŠ” ì „ì²´ íŒŒì¼ì—ì„œ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.
    # í•˜ì§€ë§Œ 'ì„ë² ë”©(Vector)'ì€ ë³€ê²½ëœ íŒŒì¼ë§Œ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤.
    
    console.print(f"\n[bold cyan]ğŸ§  Phase 2: Parsing & Building Structure...[/bold cyan]")
    
    all_chunks_for_graph = []       # ê·¸ë˜í”„ìš© (ì „ì²´)
    chunks_to_upsert = []           # ë²¡í„° ì €ì¥ìš© (ë³€ê²½ë¶„ë§Œ)
    
    # ì „ì²´ íŒŒì¼ì„ íŒŒì‹±ì€ í•˜ë˜ (ë¹ ë¦„), ì„ë² ë”© ëŒ€ìƒë§Œ ë¶„ë¥˜
    for file_path in track(all_files, description="Parsing AST..."):
        try:
            chunks = parser.parse_file(str(file_path))
            
            # ê·¸ë˜í”„ ë¹Œë”ì—” ë¬´ì¡°ê±´ ì¶”ê°€ (ì „ì²´ ë¬¸ë§¥ í˜•ì„±)
            for chunk in chunks:
                graph_builder.add_chunk(chunk)
                all_chunks_for_graph.append(chunk)
            
            # ë²¡í„° DBì—” ë³€ê²½ëœ íŒŒì¼ë§Œ ì¶”ê°€
            if file_path in files_to_embed:
                chunks_to_upsert.extend(chunks)
                
        except Exception as e:
            console.print(f"  [red]Error parsing {file_path.name}: {e}[/red]")

    # ---------------------------------------------------------
    # 5. Vector DB ì €ì¥ (Time Saver!)
    # ---------------------------------------------------------
    if chunks_to_upsert:
        console.print(f"\n[bold green]ğŸ’¾ Phase 3: Updating Vector DB ({len(chunks_to_upsert)} chunks)...[/bold green]")
        # ì—¬ê¸°ê°€ ê°€ì¥ ëŠë¦° êµ¬ê°„ì¸ë°, ë³€ê²½ëœ íŒŒì¼ë§Œ í•˜ë¯€ë¡œ ì†ë„ íšê¸°ì  ê°œì„ 
        db.upsert_chunks(chunks_to_upsert)
    else:
        console.print("\n[dim]ğŸ’¾ Phase 3: Vector DB skipped (No new content)[/dim]")

    # ---------------------------------------------------------
    # 6. Graph DB ì €ì¥ (Full Sync)
    # ---------------------------------------------------------
    # GraphëŠ” ë¶€ë¶„ ì—…ë°ì´íŠ¸ ì‹œ ê´€ê³„ ëŠê¹€(Dangling Edges) ì²˜ë¦¬ê°€ ë³µì¡í•˜ë¯€ë¡œ 
    # ì „ì²´ ì¬ìƒì„±(Full Re-indexing) ì „ëµì„ ìœ ì§€í•©ë‹ˆë‹¤. (ë¹„ìš© ì €ë ´)
    console.print(f"\n[bold magenta]ğŸ•¸ï¸ Phase 4: Syncing Graph DB...[/bold magenta]")
    
    call_graph = graph_builder.build_call_graph()
    
    # Memgraph ì´ˆê¸°í™” í›„ ì „ì²´ ë…¸ë“œ/ì—£ì§€ ë‹¤ì‹œ ì“°ê¸°
    graph_store.clear_all_data()
    graph_store.save_graph_data(all_chunks_for_graph, call_graph.edges)

    # ---------------------------------------------------------
    # 7. ìƒíƒœ ì €ì¥
    # ---------------------------------------------------------
    save_state(current_state)
    console.print("\n[bold blue]âœ¨ Incremental Ingest Complete![/bold blue]")

if __name__ == "__main__":
    main()