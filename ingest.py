import os
import sys
import json
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import track

# Qdrant Filter ê´€ë ¨ ì„í¬íŠ¸ (ì‚­ì œìš©)
from qdrant_client.models import Filter, FieldCondition, MatchText

from src.parser import ASTParser
from src.graph_builder import GraphBuilder
from src.database import VectorStore
from src.graph_store import GraphStore
from src.path_utils import should_skip_path

load_dotenv()
console = Console()

STATE_FILE = ".ingest_state.json"

# â­ [ìˆ˜ì •] ì§€ì›í•  í™•ì¥ì ëª©ë¡ì— C#ê³¼ XAML ì¶”ê°€
SUPPORTED_EXTENSIONS = {'.py', '.cs', '.xaml'}

def calculate_file_hash(filepath: Path) -> str:
    """íŒŒì¼ ë‚´ìš©ì˜ MD5 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except FileNotFoundError:
        return ""

def load_state() -> dict:
    """ì´ì „ ì¸ë±ì‹± ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_state(state: dict):
    """í˜„ì¬ ì¸ë±ì‹± ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)

def main():
    # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
    COLLECTION_NAME = os.getenv("COLLECTION_NAME", "codebase_v1")
    SOURCE_PATH = os.getenv("SOURCE_CODE_PATH", "./")
    
    # 1. ëª¨ë¸ ë° DB ì´ˆê¸°í™”
    # â­ [ìˆ˜ì •] database.pyê°€ encoder ì¸ìë¥¼ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ìƒì„±ì í˜¸ì¶œ
    db = VectorStore() 
    
    graph_store = GraphStore()
    graph_builder = GraphBuilder()
    
    # 2. íŒŒì¼ ìŠ¤ìº” ë° ë³€ê²½ ê°ì§€
    console.print("ğŸ” Phase 1: Detecting Changes...")
    
    source_path = Path(SOURCE_PATH).resolve()
    old_state = load_state()
    current_state = {}
    
    files_to_process = []
    all_chunks_for_graph = [] # ê·¸ë˜í”„ DB ì „ì²´ ë™ê¸°í™”ë¥¼ ìœ„í•´ ëª¨ë“  ì²­í¬ ì¶”ì ìš© (ì„ íƒì‚¬í•­)

    # íŒŒì¼ íƒìƒ‰ ë¡œì§
    for root, dirs, files in os.walk(source_path):
        root_path = Path(root)
        
        if should_skip_path(root_path, source_path):
            continue
            
        for file in files:
            file_path = root_path / file
            
            if should_skip_path(file_path, source_path):
                continue
            
            # â­ [ìˆ˜ì •] í™•ì¥ì í•„í„°ë§ ë¡œì§ ì¶”ê°€ (.py, .cs, .xaml)
            if file_path.suffix.lower() not in SUPPORTED_EXTENSIONS:
                continue
                
            # í•´ì‹œ ê³„ì‚°
            file_hash = calculate_file_hash(file_path)
            rel_path = str(file_path.relative_to(source_path))
            
            current_state[rel_path] = file_hash
            
            # ë³€ê²½ë˜ì—ˆê±°ë‚˜ ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼ì¸ì§€ í™•ì¸
            if rel_path not in old_state or old_state[rel_path] != file_hash:
                files_to_process.append(str(file_path))

    # ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬
    deleted_files = set(old_state.keys()) - set(current_state.keys())
    if deleted_files:
        console.print(f"ğŸ—‘ï¸  Found {len(deleted_files)} deleted files.")
        # ì‚­ì œ ë¡œì§ì€ ê¸°ì¡´ database.pyì— êµ¬í˜„ë˜ì–´ ìˆë‹¤ë©´ í˜¸ì¶œ (í˜„ì¬ëŠ” ìƒëµ)

    # ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ (ë‹¨, ê·¸ë˜í”„ ì¬êµ¬ì¶• ê°•ì œ ì˜µì…˜ì´ ìˆë‹¤ë©´ ì§„í–‰)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë³€ê²½ì‚¬í•­ ì—†ìœ¼ë©´ ì¢…ë£Œ ì²˜ë¦¬
    if not files_to_process and not deleted_files:
        console.print("\nâœ… No changes detected. System is up to date.")
        save_state(current_state)
        return

    console.print(f"ğŸ“¦ Found {len(files_to_process)} files to process.")

    # 3. íŒŒì‹± (Parsing)
    console.print(f"\n[bold yellow]ğŸ”¨ Phase 2: Parsing Code...[/bold yellow]")
    
    parser = ASTParser()
    chunks_to_upsert = []
    
    # íŠ¸ë™í‚¹í•˜ë©° íŒŒì‹±
    for filepath in track(files_to_process, description="Parsing files..."):
        # â­ [ìˆ˜ì •] parser.pyê°€ ì´ì œ ë‚´ë¶€ì ìœ¼ë¡œ í™•ì¥ìë¥¼ ì²´í¬í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ í˜¸ì¶œ
        file_chunks = parser.parse(filepath)
        chunks_to_upsert.extend(file_chunks)

    # ê·¸ë˜í”„ ë¹Œë”ì—ëŠ” 'ì „ì²´' ì²­í¬ë¥¼ ë„£ì–´ì•¼ ê´€ê³„ê°€ ì •í™•í•˜ì§€ë§Œ, 
    # ì—¬ê¸°ì„œëŠ” 'ë³€ê²½ëœ' ì²­í¬ë§Œ ì¶”ê°€í•˜ì—¬ ì ì§„ì  ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    # (ë§Œì•½ ì „ì²´ ê´€ê³„ë¥¼ ë‹¤ì‹œ ë§ºê³  ì‹¶ë‹¤ë©´ ì „ì²´ íŒŒì¼ì„ ë‹¤ì‹œ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤)
    for chunk in chunks_to_upsert:
        graph_builder.add_chunk(chunk)
        all_chunks_for_graph.append(chunk)

    console.print(f"   > Generated {len(chunks_to_upsert)} new chunks.")

    # 4. ë²¡í„° DB ì—…ì„œíŠ¸ (Vector DB Upsert)
    if chunks_to_upsert:
        console.print(f"\n[bold green]ğŸ’¾ Phase 3: Updating Vector DB...[/bold green]")
        db.upsert_chunks(chunks_to_upsert)
    
    # 5. ê·¸ë˜í”„ DB ë™ê¸°í™” (Graph DB Sync)
    # ë³€ê²½ëœ ë¶€ë¶„ì— ëŒ€í•´ì„œë§Œ ê·¸ë˜í”„ ê´€ê³„ ìƒì„±
    if chunks_to_upsert:
        console.print(f"\n[bold magenta]ğŸ•¸ï¸ Phase 4: Syncing Graph DB...[/bold magenta]")
        call_graph = graph_builder.build_call_graph()
        graph_store.save_graph_data(chunks_to_upsert, call_graph.edges)

    # 6. ìƒíƒœ ì €ì¥
    save_state(current_state)
    console.print("\n[bold blue]âœ¨ Ingest Complete![/bold blue]")

if __name__ == "__main__":
    main()